{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data exploratory - Initial\n",
    "\n",
    "### this notebook is in charge of pre-processing transaction data, removing invalid data and splitting them into proper subfile sets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 0. Initialisation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import modules.file_helper as file_help\n",
    "import modules.data_helper as data_help"
   ]
  },
  {
   "source": [
    "### 1. Load dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data already downloaded\n"
     ]
    }
   ],
   "source": [
    "#load DVF (2019) from https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/ if not yet loaded\n",
    "file_help.initialize_sells_referential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data in memory\n",
    "global_data = pd.read_csv(os.path.join(file_help.DATA_IN_FOLDER, file_help.FILE_NAME_IN), encoding='utf-8', sep='|', decimal=',')"
   ]
  },
  {
   "source": [
    "### 2. Exploratory Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                count           mean           std      min  \\\n",
      "Reference document                1.0    1110.000000           NaN  1110.00   \n",
      "2 Articles CGI                    1.0       1.000000           NaN     1.00   \n",
      "3 Articles CGI                    1.0      80.000000           NaN    80.00   \n",
      "4 Articles CGI                    1.0     119.000000           NaN   119.00   \n",
      "No disposition              2535792.0       1.199567  7.401709e+00     1.00   \n",
      "Valeur fonciere             2506530.0  971597.040251  7.856914e+06     0.01   \n",
      "No voie                     1510153.0     727.027501  2.076794e+03     1.00   \n",
      "Code postal                 2507467.0   51976.389759  2.730259e+04  1000.00   \n",
      "Code commune                2535791.0     208.996898  1.670737e+02     0.00   \n",
      "Prefixe de section           126834.0     459.487259  3.260986e+02     1.00   \n",
      "No plan                     2535790.0     403.790974  5.546665e+02     1.00   \n",
      "Surface Carrez du 2eme lot    54275.0      63.516572  6.490057e+01     0.13   \n",
      "Surface Carrez du 3eme lot     5214.0      73.177908  9.046654e+01     0.85   \n",
      "4eme lot                       9236.0     113.200195  5.230737e+02     2.00   \n",
      "Surface Carrez du 4eme lot     1415.0      86.964148  1.462351e+02     1.00   \n",
      "5eme lot                       4427.0     113.961373  4.529716e+02     2.00   \n",
      "Surface Carrez du 5eme lot      609.0      94.380181  1.132562e+02     0.89   \n",
      "Nombre de lots              2535790.0       0.394333  8.086632e-01     0.00   \n",
      "Code type local             1353332.0       1.989685  9.438622e-01     1.00   \n",
      "Identifiant local                 0.0            NaN           NaN      NaN   \n",
      "Surface reelle bati         1351615.0      87.756540  8.728312e+02     0.00   \n",
      "Nombre pieces principales   1351615.0       2.430351  2.083948e+00     0.00   \n",
      "Surface terrain             1742832.0    3092.530863  1.364342e+04     0.00   \n",
      "\n",
      "                                  25%       50%          75%           max  \n",
      "Reference document           1110.000    1110.0    1110.0000  1.110000e+03  \n",
      "2 Articles CGI                  1.000       1.0       1.0000  1.000000e+00  \n",
      "3 Articles CGI                 80.000      80.0      80.0000  8.000000e+01  \n",
      "4 Articles CGI                119.000     119.0     119.0000  1.190000e+02  \n",
      "No disposition                  1.000       1.0       1.0000  6.940000e+02  \n",
      "Valeur fonciere             60000.000  149000.0  269000.0000  2.086000e+09  \n",
      "No voie                         8.000      24.0      87.0000  9.999000e+03  \n",
      "Code postal                 31230.000   50200.0   77144.0000  9.749000e+04  \n",
      "Code commune                   76.000     174.0     300.0000  9.090000e+02  \n",
      "Prefixe de section            166.000     331.0     829.0000  9.500000e+02  \n",
      "No plan                        87.000     223.0     504.0000  9.762000e+03  \n",
      "Surface Carrez du 2eme lot     42.750      60.6      76.0400  8.630000e+03  \n",
      "Surface Carrez du 3eme lot     38.105      60.6      85.1875  3.583640e+03  \n",
      "4eme lot                        7.000      24.0      68.0000  1.701400e+04  \n",
      "Surface Carrez du 4eme lot     30.510      61.9     100.2750  3.208900e+03  \n",
      "5eme lot                        8.000      27.0      73.0000  1.104600e+04  \n",
      "Surface Carrez du 5eme lot     23.790      65.0     116.1000  1.029550e+03  \n",
      "Nombre de lots                  0.000       0.0       1.0000  1.750000e+02  \n",
      "Code type local                 1.000       2.0       3.0000  4.000000e+00  \n",
      "Identifiant local                 NaN       NaN          NaN           NaN  \n",
      "Surface reelle bati            15.000      60.0      94.0000  3.129620e+05  \n",
      "Nombre pieces principales       0.000       3.0       4.0000  6.700000e+01  \n",
      "Surface terrain               233.000     610.0    1885.0000  1.662560e+06  \n",
      "Nb records 2535792\n",
      "Nb columns 43\n",
      "Data types [dtype('O') dtype('float64') dtype('int64')]\n",
      "Nb empty columns 1\n",
      "Initial report already generated at c:\\prairie\\projet8-v2\\Immothep\\data\\reports\\rapport_initial.html\n"
     ]
    }
   ],
   "source": [
    "#basic stats about data\n",
    "print(global_data.describe().transpose())\n",
    "# Check the number of data points in the data set\n",
    "print(f'Nb records {len(global_data)}')\n",
    "# Check the number of features in the data set\n",
    "print(f'Nb columns {len(global_data.columns)}')\n",
    "# Check the data types\n",
    "print(f'Data types {global_data.dtypes.unique()}')\n",
    "\n",
    "#count empty columns\n",
    "tmp_data = global_data.dropna(axis = 1, how ='all') \n",
    "print(f'Nb empty columns {len(global_data.columns) - len(tmp_data.columns)}')\n",
    "del(tmp_data)\n",
    "\n",
    "#deep analysis\n",
    "profil = ProfileReport(global_data)\n",
    "\n",
    "#generate profiling report. If not exists. This can take more than 10 minutes\n",
    "#set True to force update of file\n",
    "force = False\n",
    "\n",
    "if os.path.exists(os.path.join( file_help.DATA_REPORTS_FOLDER ,'rapport_initial.html')) and not force: \n",
    "    print('Initial report already generated at ' + os.path.join( file_help.DATA_REPORTS_FOLDER ,'rapport_initial.html'))    \n",
    "else :\n",
    "    profil.to_file(output_file= os.path.join( file_help.DATA_REPORTS_FOLDER ,'rapport_initial.html'))\n",
    "\n",
    "del(profil)"
   ]
  },
  {
   "source": [
    "### 3. Global cleanup of the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New number of column 42/43\n",
      "New number of row 2439669/2535792\n"
     ]
    }
   ],
   "source": [
    "#clean up data - remove empty columns\n",
    "curated_data = global_data.dropna(axis = 1, how ='all') \n",
    "print(f'New number of column {len(curated_data.columns)}/{len(global_data.columns)}')\n",
    "\n",
    "#remove duplicated rows\n",
    "curated_data.drop_duplicates(inplace=True)\n",
    "print(f'New number of row {len(curated_data)}/{len(global_data)}')\n"
   ]
  },
  {
   "source": [
    "#### > Create property referential (house, appartment, ground, ...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create property referential\n",
    "global_property_type = curated_data[['Code type local','Type local']]\n",
    "global_property_type.drop_duplicates(inplace=True)\n",
    "global_property_type.dropna(inplace=True)\n",
    "\n",
    "#add 'Autre' property type\n",
    "global_property_type.loc[len(global_property_type)] = [len(global_property_type)+1,'Autre']\n",
    "\n",
    "global_property_type.sort_values(by=['Code type local'], inplace=True)\n",
    "global_property_type.set_index('Code type local')\n",
    "global_property_type['Code type local'] = global_property_type['Code type local'].astype(int)\n",
    "\n",
    "global_property_type.to_csv(os.path.join(file_help.DATA_OUT_FOLDER, 'property_type_referential.csv'), index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                Valeur fonciere\n",
       "Nombre de lots                 \n",
       "0.0                     1636095\n",
       "1.0                      615030"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Valeur fonciere</th>\n    </tr>\n    <tr>\n      <th>Nombre de lots</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>1636095</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>615030</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# drop sells with more than one lot\n",
    "curated_data.drop(curated_data.loc[curated_data[\"Nombre de lots\"]> 1].index, inplace=True)\n",
    "curated_data.groupby(\"Nombre de lots\")[['Valeur fonciere']].count().sort_values(\"Nombre de lots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "New number of column 18/43\nNew number of records 2251126/2535792\n"
     ]
    }
   ],
   "source": [
    "#drop unused columns\n",
    "curated_data.drop(columns=['No disposition','Date mutation','No voie','B/T/Q','Type de voie','Code voie','Voie','Prefixe de section','Section','No plan','No Volume','1er lot','Surface Carrez du 1er lot','2eme lot','Surface Carrez du 2eme lot','3eme lot','Surface Carrez du 3eme lot','4eme lot','Surface Carrez du 4eme lot','5eme lot','Surface Carrez du 5eme lot','Nombre de lots','Commune', 'Type local'], inplace=True, errors='ignore')\n",
    "\n",
    "#drop rows with empty sales costs\n",
    "curated_data.dropna(subset = ['Valeur fonciere'], inplace = True) \n",
    "\n",
    "print(f'New number of column {len(curated_data.columns)}/{len(global_data.columns)}')\n",
    "print(f'New number of records {len(curated_data)}/{len(global_data)}')"
   ]
  },
  {
   "source": [
    "#### > from additionnal referential, create our own to get gps coordinates and insee code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "#prepare basic file for correspondance between postal code and code INSEE\n",
    "#it will be easier to find code INSEE from a town name or postal code\n",
    "if os.path.exists(os.path.join(file_help.DATA_OUT_FOLDER, 'correspondance-code-insee-code-postal_basic.csv')):\n",
    "    print('file already exists')\n",
    "else:\n",
    "    cp = pd.read_csv(os.path.join(file_help.DATA_IN_FOLDER, 'correspondance-code-insee-code-postal.csv'), encoding='utf-8', sep=';', usecols=['Code INSEE','Code Postal','Commune','Département','Région'])\n",
    "    cp.to_csv(os.path.join(file_help.DATA_OUT_FOLDER, 'correspondance-code-insee-code-postal_basic.csv'), sep=';', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gps coordinates referential already exists at c:\\prairie\\projet8-v2\\Immothep\\data\\in\\correspondance-code-insee-code-postal.csv\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(file_help.DATA_OUT_FOLDER, 'coord_gps_referential.csv')):\n",
    " print('gps coordinates referential already exists at '+ os.path.join(file_help.DATA_IN_FOLDER, 'correspondance-code-insee-code-postal.csv'))\n",
    "else :\n",
    "    #load insee referential\n",
    "    cp = pd.read_csv(os.path.join(file_help.DATA_IN_FOLDER, 'correspondance-code-insee-code-postal.csv'), encoding='utf-8', sep=';', usecols=['Code Commune', 'Code Département', 'Code Postal', 'geo_point_2d', 'Code INSEE'])\n",
    "\n",
    "    # Create two lists for the loop results to be placed\n",
    "    lat = []\n",
    "    lon = []\n",
    "\n",
    "    # For each row in a varible,\n",
    "    for row in cp['geo_point_2d']:\n",
    "        lat.append(row.split(',')[0])\n",
    "        lon.append(row.split(',')[1])\n",
    "    \n",
    "    # Create two new columns from lat and lon\n",
    "    cp['Latitude'] = lat\n",
    "    cp['Longitude'] = lon\n",
    "\n",
    "    #split multiple cp in one row to n rows\n",
    "    df = cp['Code Postal'].str.split('/').apply(Series, 1).stack()\n",
    "    df.index = df.index.droplevel(-1) # to line up with df's index\n",
    "    df.name = 'Code Postal'\n",
    "    del(cp['Code Postal'])\n",
    "    cp = cp.join(df)\n",
    "    \n",
    "    cp.to_csv(os.path.join(file_help.DATA_OUT_FOLDER, 'coord_gps_referential.csv'), sep=';', index=False, columns= ['Code INSEE', 'Code Postal', 'Code Commune', 'Code Département', 'Latitude', 'Longitude'])\n",
    "    \n",
    "    del(lat, lon)    \n",
    "    del(cp)\n",
    "    del(df)\n"
   ]
  },
  {
   "source": [
    "#### > Fill-up missing postal code then add gps coordinates to sells"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cities referential\n",
    "cp = pd.read_csv(os.path.join(file_help.DATA_OUT_FOLDER, 'coord_gps_referential.csv'), encoding='utf-8', sep=';')\n",
    "\n",
    "#force data type for the merge matching\n",
    "cp['Code Commune'] = cp['Code Commune'].astype(int)\n",
    "cp['Code Département'] = cp['Code Département'].astype(str)\n",
    "\n",
    "#one row has no Code commune\n",
    "curated_data.dropna(subset = ['Code commune'], inplace=True) \n",
    "curated_data['Code commune'] = curated_data['Code commune'].astype(int)\n",
    "curated_data['Code departement'] = curated_data['Code departement'].astype(str)\n",
    "\n",
    "curated_data = pd.merge(curated_data, cp, how='inner', left_on=['Code commune','Code departement'], right_on = ['Code Commune','Code Département'])\n",
    "\n",
    "curated_data.drop(columns=['Code INSEE', 'Code postal', 'Code departement', 'Code commune','Code Commune', 'Code Département', 'Code Postal'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cleaned rows : 228953\n0          1.0\n1          1.0\n2          1.0\n3          5.0\n4          5.0\n          ... \n2407036    4.0\n2407037    4.0\n2407038    4.0\n2407039    4.0\n2407040    2.0\nName: Code type local, Length: 2178088, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Clear data without transaction type - they cannot be manipulated because they cannot be identified\n",
    "counter = len(curated_data)\n",
    "curated_data.drop(curated_data[(pd.isna(curated_data['Code type local'])) & (pd.isna(curated_data['Nature culture']))].index, inplace=True)\n",
    "print(f'Cleaned rows : {counter - len(curated_data)}')\n",
    "\n",
    "#set 'Autre' value to empty 'Code type local'\n",
    "other_type = int(global_property_type[global_property_type['Type local'] == 'Autre']['Code type local'])\n",
    "\n",
    "curated_data['Code type local'] = curated_data['Code type local'].fillna(other_type)\n",
    "print(curated_data['Code type local'])\n",
    "\n",
    "curated_data.drop(columns=['Nature culture', 'Nature culture speciale'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some sells have wrong number of rooms. Fix it\n",
    "curated_data['surface_group'] = np.nan\n",
    "#group sells by surfaces\n",
    "curated_data.loc[curated_data['Code type local'].isin([1, 2]), 'surface_group'] = np.round( (curated_data[curated_data['Code type local'].isin([1, 2])]['Surface reelle bati']/10))*10\n",
    "\n",
    "#compute average number of rooms for each groups\n",
    "rooms = pd.DataFrame( columns=['surface_group','nb_rooms','Code type local'])\n",
    "#only compute average for Homes and appartments\n",
    "for type_local in range(1,3):\n",
    "    for surface_group in curated_data[curated_data['Code type local'] == type_local]['surface_group'].unique():\n",
    "        nb_rooms = data_help.get_room_mean(surface_group, type_local, curated_data)\n",
    "        rooms = rooms.append({'surface_group': surface_group, 'nb_rooms': nb_rooms, 'Code type local' : type_local},ignore_index=True )\n",
    "\n",
    "curated_data = pd.merge(curated_data, rooms, how='left', on=['surface_group', 'Code type local'])\n",
    "curated_data['delta'] = np.nan\n",
    "\n",
    "#only replace number of room if difference is more than 50% of the initial value\n",
    "curated_data.loc[curated_data['Code type local'].isin([1, 2]), 'delta'] = (np.abs(curated_data[curated_data['Code type local'].isin([1, 2])]['Nombre pieces principales'] - curated_data[curated_data['Code type local'].isin([1, 2])]['nb_rooms']) > curated_data[curated_data['Code type local'].isin([1, 2])]['Nombre pieces principales'] /2)\n",
    "\n",
    "curated_data['Nombre pieces principales'] = np.where(curated_data['delta'], curated_data['nb_rooms'], curated_data['Nombre pieces principales'])\n",
    "\n",
    "#drop computed data\n",
    "curated_data.drop(columns=['delta', 'nb_rooms', 'surface_group'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete missing data with 0\n",
    "curated_data[['Surface reelle bati','Nombre pieces principales', 'Surface terrain']] = curated_data[['Surface reelle bati','Nombre pieces principales', 'Surface terrain']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Maison - 513516\n",
      "Appartement - 306925\n",
      "Dépendance - 346028\n",
      "Local industriel. commercial ou assimilé - 90393\n",
      "Autre - 921226\n"
     ]
    }
   ],
   "source": [
    "#force data type\n",
    "curated_data['Code type local'] = curated_data['Code type local'].astype(int)\n",
    "curated_data['Surface reelle bati'] = curated_data['Surface reelle bati'].astype(int)\n",
    "curated_data['Nombre pieces principales'] = curated_data['Nombre pieces principales'].astype(int)\n",
    "curated_data['Surface terrain'] = curated_data['Surface terrain'].astype(int)\n",
    "curated_data['Latitude'] = curated_data['Latitude'].astype(float)\n",
    "curated_data['Longitude'] = curated_data['Longitude'].astype(float)\n",
    "\n",
    "#reorder columns\n",
    "curated_data = curated_data[['Nature mutation', 'Code type local', 'Valeur fonciere', 'Latitude', 'Longitude', 'Surface reelle bati', 'Nombre pieces principales', 'Surface terrain']]\n",
    " \n",
    "#save data by property type\n",
    "for type in global_property_type.values:\n",
    "    property_by_type = curated_data[curated_data['Code type local'] == type[0]]\n",
    "    print(type[1] + ' - ' + str(len(property_by_type)))\n",
    "    filename = str.format(f'{type[1]}_valeursfoncieres.csv')\n",
    "    property_by_type.to_csv(os.path.join(file_help.DATA_CURATED_FOLDER, filename), sep=';', index=False )\n",
    "    del(property_by_type)\n"
   ]
  },
  {
   "source": [
    "### 5. Global cleanup - post analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Summarize dataset: 100%|██████████| 22/22 [05:18<00:00, 14.47s/it, Completed]\n",
      "Generate report structure: 100%|██████████| 1/1 [00:10<00:00, 10.64s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:07<00:00,  7.25s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#deep analysis\n",
    "profil = ProfileReport(curated_data)\n",
    "\n",
    "#generate profiling report after first cleaning, if not exists. This can take almost 5 minutes\n",
    "#set True to force update of file\n",
    "force = False\n",
    "\n",
    "if os.path.exists(os.path.join( file_help.DATA_REPORTS_FOLDER ,'rapport_final.html')) and not force:\n",
    "    print('Final report already generated at ' + os.path.join( file_help.DATA_REPORTS_FOLDER ,'rapport_final.html'))   \n",
    "else :\n",
    "    profil.to_file(output_file= os.path.join( file_help.DATA_REPORTS_FOLDER ,'rapport_final.html'))\n",
    "\n",
    "del(profil)"
   ]
  }
 ]
}